{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuniorProject21/DrugDiscovery/blob/main/Drug_discovery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset"
      ],
      "metadata": {
        "id": "wNeQbB1sAbqA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIiYz1P8FXHg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "# Install the official RDKit package\n",
        "!pip -q install rdkit\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import rdFingerprintGenerator\n",
        "from rdkit.DataStructs import ConvertToNumpyArray\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2RrdJcnWw2Z"
      },
      "outputs": [],
      "source": [
        "!pip -q install lazypredict\n",
        "\n",
        "from lazypredict.Supervised import LazyClassifier\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEDWBzxDIUZB"
      },
      "source": [
        "# Data Acquisition and Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuFBchHLFm0x"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "df_raw = pd.read_csv(\"/content/drive/MyDrive/dataset.csv\", sep=',', low_memory=False)\n",
        "\n",
        "# Select relevant columns\n",
        "cols = ['BindingDB Reactant_set_id', 'Ligand SMILES', 'IC50 (nM)',\n",
        "        'Target Source Organism According to Curator or DataSource']\n",
        "df = df_raw[cols].copy()\n",
        "\n",
        "# Filter for Human targets and drop rows with missing values\n",
        "df = df[df['Target Source Organism According to Curator or DataSource'].isin(['Homo sapiens', 'Human'])]\n",
        "df = df.dropna(subset=['IC50 (nM)', 'Ligand SMILES'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcOz-uFra7cX"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJjm6B9MGBZ1"
      },
      "outputs": [],
      "source": [
        "# 1. Clean IC50 and Organism\n",
        "def clean_data(df):\n",
        "    # Parse IC50 strings (removing <, > and converting to float)\n",
        "    df['IC50_nM_clean'] = df['IC50 (nM)'].astype(str).str.replace(r'[<>]', '', regex=True)\n",
        "    df['IC50_nM_clean'] = pd.to_numeric(df['IC50_nM_clean'], errors='coerce')\n",
        "\n",
        "    # Drop NaNs and filter for Human targets\n",
        "    df = df.dropna(subset=['IC50_nM_clean', 'Ligand SMILES'])\n",
        "    df = df[df['Target Source Organism According to Curator or DataSource'].isin(['Homo sapiens', 'Human'])].copy()\n",
        "\n",
        "    # 2. Calculate pIC50 (Standard for ML)\n",
        "    # Ensure no zero values before log transformation\n",
        "    df = df[df['IC50_nM_clean'] > 0].copy()\n",
        "    df['pIC50'] = -np.log10(df['IC50_nM_clean'] * 1e-9)\n",
        "\n",
        "    # 3. Classify Activity (Active = 1, Inactive = 0)\n",
        "    df['activity'] = np.where(df['IC50_nM_clean'] <= 1000.0, 1, 0)\n",
        "\n",
        "    return df\n",
        "\n",
        "df = clean_data(df)\n",
        "print(f\"Dataset cleaned. Active count: {df.activity.sum()} | Total: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bb31205"
      },
      "outputs": [],
      "source": [
        "# Save the cleaned dataframe to Google Drive\n",
        "file_path = \"/content/drive/MyDrive/dataset_clean.csv\"\n",
        "df.to_csv(file_path, index=False)\n",
        "print(f\"Cleaned dataset saved successfully to: {file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6_YfZ_6Kmhv"
      },
      "source": [
        "# Lipinski descriptors (not mandatory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNqseNX1b9y8"
      },
      "outputs": [],
      "source": [
        "## maybe added later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S05SDHbJRfLD"
      },
      "source": [
        "# Fingerprints\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97rL7i4XbnFP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import rdFingerprintGenerator\n",
        "from rdkit.DataStructs import ConvertToNumpyArray\n",
        "\n",
        "# -------- Optimal fingerprint settings (from your experiment) --------\n",
        "RADIUS = 3\n",
        "NBITS = 4096\n",
        "\n",
        "gen = rdFingerprintGenerator.GetMorganGenerator(radius=RADIUS, fpSize=NBITS)\n",
        "\n",
        "def get_fp(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    fp = gen.GetFingerprint(mol)\n",
        "    arr = np.zeros((NBITS,), dtype=np.int8)\n",
        "    ConvertToNumpyArray(fp, arr)\n",
        "    return arr\n",
        "\n",
        "# df must already exist and contain these columns\n",
        "# df[\"Ligand SMILES\"], df[\"activity\"] (and/or IC50/pIC50 columns depending on your notebook)\n",
        "\n",
        "smiles_all = df[\"Ligand SMILES\"].astype(str).tolist()\n",
        "fps = [get_fp(s) for s in smiles_all]\n",
        "\n",
        "valid_indices = [i for i, f in enumerate(fps) if f is not None]\n",
        "print(\"Valid molecules:\", len(valid_indices), \" / \", len(df))\n",
        "\n",
        "# X = fingerprints only (aligned with df_final)\n",
        "X = np.stack([fps[i] for i in valid_indices]).astype(np.float32)\n",
        "df_final = df.iloc[valid_indices].reset_index(drop=True)\n",
        "\n",
        "# Labels aligned with X\n",
        "y = df_final[\"activity\"].astype(int).to_numpy()\n",
        "\n",
        "print(\"X shape:\", X.shape, \" y shape:\", y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "# ---- Full fingerprint dataframe (4096 bits) ----\n",
        "fp_cols_full = [f'fp_{i}' for i in range(NBITS)]\n",
        "df_fps_full = pd.DataFrame(X.astype(np.int8), columns=fp_cols_full)\n",
        "df_ready_full = pd.concat([df_final.reset_index(drop=True), df_fps_full], axis=1)\n",
        "\n",
        "# ---- Remove zero-variance bits ----\n",
        "selector = VarianceThreshold(threshold=0.0)\n",
        "X_reduced = selector.fit_transform(X)  # this becomes your final Xf for modeling\n",
        "\n",
        "mask = selector.get_support()\n",
        "kept_cols = [fp_cols_full[i] for i, keep in enumerate(mask) if keep]\n",
        "\n",
        "df_fps_reduced = pd.DataFrame(X_reduced.astype(np.int8), columns=kept_cols)\n",
        "df_ready_reduced = pd.concat([df_final.reset_index(drop=True), df_fps_reduced], axis=1)\n",
        "\n",
        "print(f\"Features reduced from {NBITS} to {X_reduced.shape[1]} bits.\")\n",
        "\n",
        "# ---- Save to Drive ----\n",
        "base = \"/content/drive/MyDrive\"\n",
        "full_path = f\"{base}/dataset_features_ECFP6_r3_4096_FULL.csv\"\n",
        "reduced_path = f\"{base}/dataset_features_ECFP6_r3_4096_REDUCED.csv\"\n",
        "mask_path = f\"{base}/fingerprint_keep_mask_ECFP6_r3_4096.npy\"\n",
        "\n",
        "df_ready_full.to_csv(full_path, index=False)\n",
        "df_ready_reduced.to_csv(reduced_path, index=False)\n",
        "np.save(mask_path, mask)\n",
        "\n",
        "print(\"Saved full dataset:\", full_path)\n",
        "print(\"Saved reduced dataset:\", reduced_path)\n",
        "print(\"Saved keep-mask:\", mask_path)\n",
        "\n",
        "# Choose which X to use for modeling:\n",
        "Xf = X_reduced  # recommended"
      ],
      "metadata": {
        "id": "7uxKW838nlNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XI2W3aSlJ8os"
      },
      "outputs": [],
      "source": [
        "print(X.shape)\n",
        "print(np.unique(X))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaffold split\n",
        "\n",
        "A scaffold split is a method in molecular machine learning and cheminformatics that divides a chemical dataset based on the core structural framework (scaffold) of molecules. By ensuring that no molecule sharing a scaffold in the training set appears in the validation or test sets, this method simulates real world drug discovery scenarios, testing model generalizability to new chemical series"
      ],
      "metadata": {
        "id": "n6hZJjlNYn-H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnvkGpiCLaGB"
      },
      "outputs": [],
      "source": [
        "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
        "from collections import defaultdict\n",
        "\n",
        "def murcko(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    scaf = MurckoScaffold.GetScaffoldForMol(mol)\n",
        "    return Chem.MolToSmiles(scaf, isomericSmiles=False)\n",
        "\n",
        "smiles_list = df_final[\"Ligand SMILES\"].astype(str).tolist()\n",
        "scaffolds = [murcko(s) for s in smiles_list]\n",
        "\n",
        "scaf_to_idx = defaultdict(list)\n",
        "for i, sc in enumerate(scaffolds):\n",
        "    if sc is not None:\n",
        "        scaf_to_idx[sc].append(i)\n",
        "\n",
        "groups = sorted(scaf_to_idx.values(), key=len, reverse=True)\n",
        "\n",
        "test_fraction = 0.2\n",
        "n_total = len(df_final)\n",
        "n_test_target = int(test_fraction * n_total)\n",
        "\n",
        "test_idx = []\n",
        "train_idx = []\n",
        "\n",
        "for g in groups:\n",
        "    # add whole scaffold group to test until target reached\n",
        "    if len(test_idx) < n_test_target:\n",
        "        test_idx.extend(g)\n",
        "    else:\n",
        "        train_idx.extend(g)\n",
        "\n",
        "train_idx = np.array(train_idx, dtype=int)\n",
        "test_idx = np.array(test_idx, dtype=int)\n",
        "\n",
        "X_train, X_test = Xf[train_idx], Xf[test_idx]\n",
        "y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "print(\"Train:\", X_train.shape, \" Test:\", X_test.shape)\n",
        "print(\"Test fraction actual:\", len(test_idx)/n_total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vzW8ldlKfbM"
      },
      "source": [
        "#Training different ML models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lazypredict.Supervised import LazyClassifier\n",
        "\n",
        "X_train_lp = X_train.astype(float)\n",
        "X_test_lp  = X_test.astype(float)\n",
        "\n",
        "clf = LazyClassifier(verbose=0, ignore_warnings=True)\n",
        "models, predictions = clf.fit(X_train_lp, X_test_lp, y_train, y_test)\n",
        "\n",
        "# Save leaderboards/results\n",
        "models_path = \"/content/drive/MyDrive/lazypredict_models_ECFP6_r3_4096.csv\"\n",
        "preds_path  = \"/content/drive/MyDrive/lazypredict_predictions_ECFP6_r3_4096.csv\"\n",
        "\n",
        "models.to_csv(models_path, index=True)\n",
        "predictions.to_csv(preds_path, index=False)\n",
        "\n",
        "print(\"Saved model leaderboard:\", models_path)\n",
        "print(\"Saved predictions table:\", preds_path)\n",
        "\n",
        "models.head(10)"
      ],
      "metadata": {
        "id": "LzM1XM6VreIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models"
      ],
      "metadata": {
        "id": "CHv_xP_wpQgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Top 10 models"
      ],
      "metadata": {
        "id": "Ps1tLwoyn42n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyzpM7G-XdDE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score,\n",
        "    accuracy_score, balanced_accuracy_score, f1_score,\n",
        "    confusion_matrix, ConfusionMatrixDisplay\n",
        ")\n",
        "\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Optional models\n",
        "try:\n",
        "    from lightgbm import LGBMClassifier\n",
        "    has_lgbm = True\n",
        "except Exception:\n",
        "    has_lgbm = False\n",
        "\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    has_xgb = True\n",
        "except Exception:\n",
        "    has_xgb = False\n",
        "\n",
        "def get_scores(model, X):\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        return model.predict_proba(X)[:, 1]\n",
        "    if hasattr(model, \"decision_function\"):\n",
        "        return model.decision_function(X)\n",
        "    return model.predict(X)\n",
        "\n",
        "models_to_eval = {\n",
        "    \"BernoulliNB\": BernoulliNB(),\n",
        "    \"BaggingClassifier\": BaggingClassifier(random_state=42),\n",
        "    \"DecisionTreeClassifier\": DecisionTreeClassifier(random_state=42),\n",
        "    \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
        "}\n",
        "\n",
        "if has_lgbm:\n",
        "    models_to_eval[\"LGBMClassifier\"] = LGBMClassifier(random_state=42)\n",
        "\n",
        "if has_xgb:\n",
        "    models_to_eval[\"XGBClassifier\"] = XGBClassifier(\n",
        "        random_state=42,\n",
        "        eval_metric=\"logloss\",\n",
        "        use_label_encoder=False\n",
        "    )\n",
        "\n",
        "rows = []\n",
        "fitted_models = {}\n",
        "\n",
        "for name, model in models_to_eval.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    fitted_models[name] = model\n",
        "\n",
        "    scores = get_scores(model, X_test)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    rows.append({\n",
        "        \"Model\": name,\n",
        "        \"ROC_AUC\": roc_auc_score(y_test, scores),\n",
        "        \"PR_AUC\": average_precision_score(y_test, scores),\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Balanced_Accuracy\": balanced_accuracy_score(y_test, y_pred),\n",
        "        \"F1\": f1_score(y_test, y_pred),\n",
        "    })\n",
        "\n",
        "auc_df = pd.DataFrame(rows).sort_values([\"ROC_AUC\", \"PR_AUC\"], ascending=False).reset_index(drop=True)\n",
        "display(auc_df)\n",
        "\n",
        "# Confusion matrix for best model (by ROC_AUC)\n",
        "best_name = auc_df.iloc[0][\"Model\"]\n",
        "best_model = fitted_models[best_name]\n",
        "\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred_best)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Inactive (0)\", \"Active (1)\"])\n",
        "disp.plot(ax=ax, values_format=\"d\", cmap=\"Blues\")   # <-- blue theme\n",
        "ax.grid(False)                                      # optional: cleaner\n",
        "plt.title(f\"Confusion Matrix (Best model: {best_name})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Best model by ROC_AUC:\", best_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Raw8myoDphny"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "top_n = 3\n",
        "top_models = auc_df[\"Model\"].head(top_n).tolist()\n",
        "\n",
        "for name in top_models:\n",
        "    model = fitted_models[name]\n",
        "    y_pred = model.predict(X_test)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    disp = ConfusionMatrixDisplay(cm, display_labels=[\"Inactive (0)\", \"Active (1)\"])\n",
        "    disp.plot(ax=ax, values_format=\"d\", cmap=\"Blues\")  # <- blue theme\n",
        "    plt.title(f\"Confusion Matrix â€” {name}\")\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXPERIMENT"
      ],
      "metadata": {
        "id": "sD0KRAdMh0-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# EXPERIMENT: Representation / Feature Engineering\n",
        "# (Safe: does NOT modify your existing train_idx/test_idx)\n",
        "# ============================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import rdFingerprintGenerator, Descriptors\n",
        "from rdkit.DataStructs import ConvertToNumpyArray\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# ---------- 0) Make sure we have a validation set ----------\n",
        "# If you already have val_idx defined, it will use it.\n",
        "# If not, it will split it from train_idx (keeping test_idx untouched).\n",
        "if \"val_idx\" not in globals():\n",
        "    train_idx_arr = np.array(train_idx, dtype=int)\n",
        "    rng = np.random.RandomState(42)\n",
        "    perm = rng.permutation(train_idx_arr)\n",
        "\n",
        "    val_size = max(1, int(0.125 * len(train_idx_arr)))  # ~12.5% of train\n",
        "    val_idx = perm[:val_size]\n",
        "    train_idx2 = perm[val_size:]\n",
        "else:\n",
        "    train_idx2 = np.array(train_idx, dtype=int)\n",
        "    val_idx = np.array(val_idx, dtype=int)\n",
        "\n",
        "print(f\"[Representation Exp] train_idx2={len(train_idx2)}  val_idx={len(val_idx)}  test_idx={len(test_idx)}\")\n",
        "\n",
        "# ---------- 1) Helpers ----------\n",
        "DESC_FUNCS = [\n",
        "    (\"MolWt\", Descriptors.MolWt),\n",
        "    (\"MolLogP\", Descriptors.MolLogP),\n",
        "    (\"TPSA\", Descriptors.TPSA),\n",
        "    (\"HBD\", Descriptors.NumHDonors),\n",
        "    (\"HBA\", Descriptors.NumHAcceptors),\n",
        "    (\"RotB\", Descriptors.NumRotatableBonds),\n",
        "    (\"RingCount\", Descriptors.RingCount),\n",
        "]\n",
        "\n",
        "def featurize(smiles_list, radius=2, nbits=2048, add_desc=False):\n",
        "    gen = rdFingerprintGenerator.GetMorganGenerator(radius=radius, fpSize=nbits)\n",
        "\n",
        "    fps = []\n",
        "    descs = []\n",
        "    bad = 0\n",
        "\n",
        "    for smi in smiles_list:\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if mol is None:\n",
        "            bad += 1\n",
        "            fp_arr = np.zeros((nbits,), dtype=np.int8)\n",
        "            fps.append(fp_arr)\n",
        "            if add_desc:\n",
        "                descs.append([0.0] * len(DESC_FUNCS))\n",
        "            continue\n",
        "\n",
        "        fp = gen.GetFingerprint(mol)\n",
        "        fp_arr = np.zeros((nbits,), dtype=np.int8)\n",
        "        ConvertToNumpyArray(fp, fp_arr)\n",
        "        fps.append(fp_arr)\n",
        "\n",
        "        if add_desc:\n",
        "            descs.append([fn(mol) for _, fn in DESC_FUNCS])\n",
        "\n",
        "    X_fp = np.vstack(fps).astype(np.float32)\n",
        "\n",
        "    if not add_desc:\n",
        "        return X_fp, bad\n",
        "\n",
        "    # Descriptors (scale only descriptors)\n",
        "    X_desc = np.array(descs, dtype=np.float32)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_desc_scaled = scaler.fit_transform(X_desc)\n",
        "\n",
        "    X_all = np.hstack([X_fp, X_desc_scaled]).astype(np.float32)\n",
        "    return X_all, bad\n",
        "\n",
        "def eval_config(X_feat, y, train_idx_used, val_idx_used):\n",
        "    model = LogisticRegression(max_iter=2000, n_jobs=-1)\n",
        "\n",
        "    X_tr, y_tr = X_feat[train_idx_used], y[train_idx_used]\n",
        "    X_va, y_va = X_feat[val_idx_used], y[val_idx_used]\n",
        "\n",
        "    model.fit(X_tr, y_tr)\n",
        "    proba = model.predict_proba(X_va)[:, 1]\n",
        "    pred = (proba >= 0.5).astype(int)\n",
        "\n",
        "    roc = roc_auc_score(y_va, proba)\n",
        "    pr  = average_precision_score(y_va, proba)\n",
        "    acc = accuracy_score(y_va, pred)\n",
        "    return roc, pr, acc\n",
        "\n",
        "# ---------- 2) Run experiment grid ----------\n",
        "smiles_list = df_final[\"Ligand SMILES\"].astype(str).tolist()\n",
        "y_arr = df_final[\"activity\"].astype(int).to_numpy()\n",
        "\n",
        "configs = [(r, b, d) for r in [2, 3] for b in [1024, 2048, 4096] for d in [False, True]]\n",
        "\n",
        "rows = []\n",
        "for radius, nbits, add_desc in configs:\n",
        "    X_feat, bad = featurize(smiles_list, radius=radius, nbits=nbits, add_desc=add_desc)\n",
        "\n",
        "    roc, pr, acc = eval_config(X_feat, y_arr, train_idx2, val_idx)\n",
        "\n",
        "    rows.append({\n",
        "        \"radius\": radius,\n",
        "        \"nbits\": nbits,\n",
        "        \"descriptors\": add_desc,\n",
        "        \"val_ROC_AUC\": roc,\n",
        "        \"val_PR_AUC\": pr,\n",
        "        \"val_Accuracy\": acc,\n",
        "        \"bad_smiles\": bad,\n",
        "        \"feature_dim\": X_feat.shape[1],\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(rows).sort_values([\"val_ROC_AUC\", \"val_PR_AUC\"], ascending=False).reset_index(drop=True)\n",
        "results_df"
      ],
      "metadata": {
        "id": "QsOQELHYE8AT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from rdkit import Chem, DataStructs\n",
        "from rdkit.Chem import AllChem, rdFingerprintGenerator\n",
        "# Added rdFingerprintGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# ---------- Settings ----------\n",
        "RADIUS = 3\n",
        "NBITS = 4096\n",
        "SEED = 42\n",
        "PAIRS = 2000\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Initialize the Morgan Generator once (Fixes the Deprecation Warning)\n",
        "mfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=RADIUS, fpSize=NBITS)\n",
        "\n",
        "smiles = df_final[\"Ligand SMILES\"].astype(str).tolist()\n",
        "y_arr = df_final[\"activity\"].astype(int).to_numpy()\n",
        "\n",
        "# ---------- Fingerprints ----------\n",
        "def morgan_fp(smi):\n",
        "    mol = Chem.MolFromSmiles(smi)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    # Use the generator instead of AllChem.GetMorganFingerprintAsBitVect\n",
        "    return mfpgen.GetFingerprint(mol)\n",
        "\n",
        "fps = []\n",
        "bad = 0\n",
        "for smi in smiles:\n",
        "    fp = morgan_fp(smi)\n",
        "    if fp is None:\n",
        "        bad += 1\n",
        "    fps.append(fp)\n",
        "\n",
        "print(\"Bad SMILES:\", bad)\n",
        "\n",
        "valid_idx = np.array([i for i, fp in enumerate(fps) if fp is not None],\n",
        "                     dtype=int)\n",
        "\n",
        "# ---------- Helper 1: random cross-pair similarity ----------\n",
        "def cross_pair_summary(train_indices, test_indices, pairs=PAIRS):\n",
        "    train_indices = [i for i in train_indices if fps[i] is not None]\n",
        "    test_indices  = [i for i in test_indices  if fps[i] is not None]\n",
        "\n",
        "    sims = []\n",
        "    for _ in range(pairs):\n",
        "        i = random.choice(train_indices)\n",
        "        j = random.choice(test_indices)\n",
        "        sims.append(DataStructs.TanimotoSimilarity(fps[i], fps[j]))\n",
        "\n",
        "    sims = np.array(sims, dtype=float)\n",
        "    return {\n",
        "        \"pairs_used\": len(sims),\n",
        "        \"mean\": float(sims.mean()),\n",
        "        \"median\": float(np.median(sims)),\n",
        "        \"p90\": float(np.percentile(sims, 90)),\n",
        "        \"max\": float(sims.max()),\n",
        "    }\n",
        "\n",
        "# ---------- Helper 2: nearest-neighbor similarity ----------\n",
        "def nn_summary(train_indices, test_indices):\n",
        "    train_indices = [i for i in train_indices if fps[i] is not None]\n",
        "    test_indices  = [i for i in test_indices  if fps[i] is not None]\n",
        "\n",
        "    train_fps = [fps[i] for i in train_indices]\n",
        "    nn_sims = []\n",
        "\n",
        "    for j in test_indices:\n",
        "        sims = DataStructs.BulkTanimotoSimilarity(fps[j], train_fps)\n",
        "        nn_sims.append(max(sims))\n",
        "\n",
        "    nn_sims = np.array(nn_sims, dtype=float)\n",
        "    return {\n",
        "        \"test_size\": len(test_indices),\n",
        "        \"mean_nn\": float(nn_sims.mean()),\n",
        "        \"median_nn\": float(np.median(nn_sims)),\n",
        "        \"p90_nn\": float(np.percentile(nn_sims, 90)),\n",
        "        \"max_nn\": float(nn_sims.max()),\n",
        "        \"pct_nn_ge_0.5\": float((nn_sims >= 0.5).mean()),\n",
        "        \"pct_nn_ge_0.7\": float((nn_sims >= 0.7).mean()),\n",
        "    }\n",
        "\n",
        "# ---------- A) Scaffold split ----------\n",
        "train_scaf = [i for i in train_idx if i in set(valid_idx)]\n",
        "test_scaf  = [i for i in test_idx  if i in set(valid_idx)]\n",
        "\n",
        "# ---------- B) Random split with SAME test size ----------\n",
        "test_ratio = len(test_scaf) / len(valid_idx)\n",
        "rand_train, rand_test = train_test_split(\n",
        "    valid_idx, test_size=test_ratio, random_state=SEED,\n",
        "    stratify=y_arr[valid_idx]\n",
        ")\n",
        "\n",
        "# ---------- Compute summaries ----------\n",
        "scaf_pairs = cross_pair_summary(train_scaf, test_scaf, pairs=PAIRS)\n",
        "rand_pairs = cross_pair_summary(rand_train, rand_test, pairs=PAIRS)\n",
        "\n",
        "scaf_nn = nn_summary(train_scaf, test_scaf)\n",
        "rand_nn = nn_summary(rand_train, rand_test)\n",
        "\n",
        "pairs_df = pd.DataFrame([\n",
        "    {\"Split\": \"Scaffold\", **scaf_pairs},\n",
        "    {\"Split\": \"Random\",   **rand_pairs},\n",
        "])\n",
        "\n",
        "nn_df = pd.DataFrame([\n",
        "    {\"Split\": \"Scaffold\", **scaf_nn},\n",
        "    {\"Split\": \"Random\",   **rand_nn},\n",
        "])\n",
        "\n",
        "print(\"\\nRandom cross-pair similarity (weak leakage signal):\")\n",
        "display(pairs_df)\n",
        "\n",
        "print(\"\\nNearest-neighbor similarity (best leakage signal):\")\n",
        "display(nn_df)"
      ],
      "metadata": {
        "id": "B1JfNEmGPKZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Data Preparation ---\n",
        "scaffold = {\n",
        "    \"mean_nn\": 0.62, \"median_nn\": 0.64, \"p90_nn\": 0.85, \"max_nn\": 0.90,\n",
        "    \"pct_nn_ge_0.5\": 0.78, \"pct_nn_ge_0.7\": 0.31\n",
        "}\n",
        "random_split = {\n",
        "    \"mean_nn\": 0.79, \"median_nn\": 0.78, \"p90_nn\": 1.00, \"max_nn\": 1.00,\n",
        "    \"pct_nn_ge_0.5\": 0.95, \"pct_nn_ge_0.7\": 0.72\n",
        "}\n",
        "\n",
        "# Values for Plot 1 (Stats)\n",
        "labels_stats = [\"Mean\", \"Median\", \"90th Pct\", \"Max\"]\n",
        "vals_scaf = [scaffold[\"mean_nn\"], scaffold[\"median_nn\"], scaffold[\"p90_nn\"], scaffold[\"max_nn\"]]\n",
        "vals_rand = [random_split[\"mean_nn\"], random_split[\"median_nn\"], random_split[\"p90_nn\"], random_split[\"max_nn\"]]\n",
        "\n",
        "# Values for Plot 2 (Thresholds/Leakage)\n",
        "labels_thr = [\"Tanimoto $\\geq$ 0.5\", \"Tanimoto $\\geq$ 0.7\"]\n",
        "vals_scaf2 = [scaffold[\"pct_nn_ge_0.5\"], scaffold[\"pct_nn_ge_0.7\"]]\n",
        "vals_rand2 = [random_split[\"pct_nn_ge_0.5\"], random_split[\"pct_nn_ge_0.7\"]]\n",
        "\n",
        "# --- Plotting Configuration ---\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), constrained_layout=True)\n",
        "color_scaf = '#3498db'  # Professional Blue\n",
        "color_rand = '#e74c3c'  # Professional Red\n",
        "width = 0.35\n",
        "x = np.arange(len(labels_stats))\n",
        "x2 = np.arange(len(labels_thr))\n",
        "\n",
        "# Helper function for data labels\n",
        "def autolabel(rects, ax):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate(f'{height:.2f}',\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3), textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "# --- Plot 1: NN Similarity Stats ---\n",
        "rects1 = ax1.bar(x - width/2, vals_scaf, width, label='Scaffold Split', color=color_scaf, edgecolor='black', alpha=0.85)\n",
        "rects2 = ax1.bar(x + width/2, vals_rand, width, label='Random Split', color=color_rand, edgecolor='black', alpha=0.85)\n",
        "\n",
        "ax1.set_ylabel('Nearest-Neighbor Tanimoto Similarity', fontsize=12)\n",
        "ax1.set_title('Test Set Proximity to Training Data', fontsize=14, fontweight='bold', pad=15)\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(labels_stats)\n",
        "ax1.set_ylim(0, 1.15) # Leave space for labels\n",
        "ax1.legend(frameon=False)\n",
        "ax1.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "autolabel(rects1, ax1)\n",
        "autolabel(rects2, ax1)\n",
        "\n",
        "# --- Plot 2: Leakage Thresholds ---\n",
        "rects3 = ax2.bar(x2 - width/2, vals_scaf2, width, label='Scaffold Split', color=color_scaf, edgecolor='black', alpha=0.85)\n",
        "rects4 = ax2.bar(x2 + width/2, vals_rand2, width, label='Random Split', color=color_rand, edgecolor='black', alpha=0.85)\n",
        "\n",
        "ax2.set_ylabel('Fraction of Test Compounds', fontsize=12)\n",
        "ax2.set_title('Potential Data Leakage Signal', fontsize=14, fontweight='bold', pad=15)\n",
        "ax2.set_xticks(x2)\n",
        "ax2.set_xticklabels(labels_thr)\n",
        "ax2.set_ylim(0, 1.15)\n",
        "ax2.legend(frameon=False)\n",
        "ax2.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "autolabel(rects3, ax2)\n",
        "autolabel(rects4, ax2)\n",
        "\n",
        "plt.savefig('improved_visualization.png', dpi=300)"
      ],
      "metadata": {
        "id": "DKRf28ogi5-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fix"
      ],
      "metadata": {
        "id": "zTtmjTatugSw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPMxpdhRxCLmpteWyqVxBKb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}